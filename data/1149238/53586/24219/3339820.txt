Học sâu (tiếng Anh: deep learning, còn gọi là học cấu trúc sâu) là một phần trong một nhánh rộng hơn các phương pháp học máy dựa trên mạng thần kinh nhân tạo kết hợp với việc học biểu diễn đặc trưng (representation learning). Việc học này có thể có giám sát, nửa giám sát hoặc không giám sát.
Mạng thần kinh nhân tạo được lấy cảm hứng từ việc xử lý thông tin và các nút giao tiếp phân tán trong hệ sinh học. Nó có nhiều khác biệt so với não sinh học. Cụ thể, mạng thần kinh nhân tạo thường có tính tĩnh và mang tính biểu tượng, trong khi não bộ của hầu hết các sinh vật sống có tính động (linh hoạt) và analog.
Học sâu thường được nhắc đến cùng với Dữ liệu lớn (Big Data) và Trí tuệ nhân tạo (AI). Đã có nhiều ứng dụng trong thực tế , đang phát triển mạnh theo sự phát triển của tốc độ máy tính đặc biệt là khả năng tính toán trên  GPU và sự tăng nhanh của dữ liệu cùng với các framework (TensorFlow hay Pytorch) làm việc xây dựng model trở nên dễ dàng hơn.
Học sâu là một phần của một họ các phương pháp học máy rộng hơn dựa trên đại diện học của dữ liệu. Một quan sát (ví dụ như, một hình ảnh) có thể được biểu diễn bằng nhiều cách như một vector của các giá trị cường độ cho mỗi điểm ảnh, hoặc một cách trừu tượng hơn như là một tập hợp các cạnh, các khu vực hình dạng cụ thể, vv. Một vài đại diện làm khiến việc học các nhiệm vụ dễ dàng hơn (ví dụ, nhận dạng khuôn mặt hoặc biểu hiện cảm xúc trên khuôn mặt) từ các ví dụ. Một trong những hứa hẹn của học sâu là thay thế các tính năng thủ công bằng các thuật toán hiệu quả đối với học không có giám sát hoặc nửa giám sát và tính năng phân cấp.
Nhiều kiến trúc học sâu khác nhau như mạng neuron sâu, mã mạng neuron tích chập sâu, mạng niềm tin sâu và mạng neuron tái phát đã được áp dụng cho các lĩnh vực như thị giác máy tính, tự động nhận dạng giọng nói, xử lý ngôn ngữ tự nhiên, nhận dạng âm thanh ngôn ngữ và tin sinh học, chúng đã được chứng minh là tạo ra các kết quả rất tốt đối với nhiều nhiệm vụ khác nhau.
Ngoài ra, học sâu đã trở thành một từ ngữ thời thượng, hay một thương hiệu của mạng neuron.


== Giới thiệu ==


=== Định nghĩa ===
Có một số cách để mô tả học sâu. Học sâu là một lớp của các thuật toán máy học mà

Sử dụng một tầng (cascade) nhiều lớp các đơn vị xử lý phi tuyến để trích tách đặc điểm và chuyển đổi. Mỗi lớp kế tiếp dùng đầu ra từ lớp trước làm đầu vào. Các thuật toán này có thể được giám sát hoặc không cần giám sát và các ứng dụng bao gồm các mô hình phân tích (không có giám sát) và phân loại (giám sát).
Dựa trên học (không có giám sát) của nhiều cấp các đặc điểm hoặc đại diện của dữ liệu. Các tính năng cao cấp bắt nguồn từ các tính năng thấp cấp hơn để tạo thành một đại diện thứ bậc.
Là một phần của lĩnh vực máy học rộng lớn hơn về việc học đại diện dữ liệu.
Học nhiều cấp độ đại diện tương ứng với các mức độ trừu tượng khác nhau; các mức độ hình thành một hệ thống phân cấp của các khái niệm.
Các định nghĩa này có điểm chung là (1) nhiều lớp các đơn vị xử lý phi tuyến và (2) học có giám sát hoặc không có giám sát của biểu diễn đặc tính ở mỗi lớp, với các lớp hình thành một hệ thống các tính năng phân cấp từ thấp đến cao cấp. Các thành phần của một lớp của đơn vị xử lý phi tuyến sử dụng một thuật toán học sâu tùy theo vấn đề cần được giải quyết. Các lớp được sử dụng trong học sâu bao gồm các lớp ẩn của một mạng neuron nhân tạo và tập các công thức mệnh đề phức tạp. Chúng cũng có thể bao gồm các biến tiềm ẩn được tổ chức thành các lớp chọn lọc trong các mô hình thể sinh (có khả năng sinh ra) sâu như các nút trong Deep Belief Networks và Deep Boltzmann Machines.
Các thuật toán học sâu tương phản với các thuật toán học nông bởi số biến đổi được tham số hóa một tín hiệu gặp phải khi nó lan truyền từ các lớp đầu vào đến lớp đầu ra, nơi một biến đổi được tham số hóa là một đơn vị xử lý có các thông số có thể huấn luyện được, chẳng hạn như trọng số và ngưỡng. Một chuỗi các biến đổi từ đầu vào đến đầu ra là một đường gán kế thừa (CAP- credit assignment path). CAP mô tả các kết nối quan hệ nhân quả tiềm năng giữa đầu vào và đầu ra và có thể thay đổi chiều dài. Đối với một mạng neuron nuôi tiến (feedforward), độ sâu của CAP, và do đó độ sâu của mạng đó, là số lượng các lớp ẩn cộng 1 (lớp đầu ra cũng là tham số hóa). Đối với mạng neuron tái phát, trong đó một tín hiệu có thể truyền thông qua một lớp nhiều hơn một lần, CAPcó khả năng không bị giới hạn chiều dài. Không có sự thống nhất chung về ngưỡng của độ sâu chia học cạn với học sâu, nhưng hầu hết các nhà nghiên cứu trong lĩnh vực đồng ý rằng học sâu có nhiều lớp phi tuyến (CAP > 2) và Schmidhuber coi CAP > 10 để là học rất sâu.


=== Khái niệm cơ bản ===
Các thuật toán học sâu dựa trên các đại diện phân phối. Giả định tiềm ẩn đằng sau các đại diện phân phối là các dữ liệu được quan sát là được tạo ra bởi sự tương tác của các yếu tố được tổ chức theo lớp. Học sâu thêm giả định rằng các lớp của các yếu tố này tương ứng với các mức độ trừu tượng hay theo thành phần. Các con số khác nhau của các lớp và kích thước của lớp có thể được sử dụng để quy định các lượng trừu tượng khác.
Học sâu khai thác ý tưởng thứ bậc các yếu tố giải thích này ở cấp cao hơn, những khái niệm trừu tượng hơn được học từ các cấp độ thấp hơn. Những kiến trúc này thường được xây dựng với một phương pháp lớp chồng lớp tham lam. Học sâu giúp để tháo gỡ những khái niệm trừu tượng này và chọn ra những đặc điểm cần thiết cho việc học.
Đối với các nhiệm vụ học có giám sát, các phương pháp học sâu sẽ tránh kỹ thuật đặc điểm (feature engineering), bằng cách dịch các dữ liệu vào các đại diện trung gian nhỏ gọn giống như các thành phần chính, và lấy được các cấu trúc lớp mà loại bỏ sự thừa thải trong đại diện.
Rất nhiều các thuật toán học sâu được áp dụng cho các nhiệm vụ học không có giám sát. Đây là một lợi ích quan trọng bởi vì dữ liệu không dán nhãn (chưa phân loại) thường phong phú hơn các dữ liệu dán nhãn. Một ví dụ của một cấu trúc sâu có thể được đào tạo theo cách không có giám sát là một mạng lưới tin sâu (deep belief network).


== Diễn giải ==
Mạng neuron sâu thường được giải thích theo cách: định lý xấp xỉ tổng quát hoặc Suy luận xác suất.


=== Diễn giải Định lý Xấp xỉ Phổ quát ===
Định lý xấp xỉ phổ quát đề cập đến khả năng của mạng neuron tiến tiếp (feedforward) với một lớp ẩn có kích thước hữu hạn đơn để xấp xỉ các hàm liên tục.
Năm 1989, là bằng chứng đầu tiên được xuất bản bởi George Cybenko cho các hàm kích hoạt hình sigma và được mở rộng đối với các kiến trúc nuôi tiến nhiều lớp vào năm 1991 bởi Kurt Hornik.


=== Diễn giải xác suất ===
Diễn giải xác suất bắt nguồn từ lĩnh vực máy học. Nó có đặc điểm suy luận, cũng như các khái niệm tối ưu hóa huấn luyện và kiểm tra, liên quan đến việc phù hợp và tổng quát hóa tương ứng. Cụ thể hơn, diễn giải xác suất sẽ xem xét kích hoạt một cách phi tuyến như là một hàm phân phối tích lũy. Xem mạng tin sâu. Diễn giải xác suất dẫn đến sự ra đời của dropout như regularizer trong mạng neuron.
Diễn giải xác suất đã được giới thiệu và phổ biến rộng rãi bởi những tiên phong như Geoff Hinton, Yoshua Bengio, Yann Le Cun, Juergen Schmidhuber.


== Lịch sử ==
Các kiến trúc học sâu, đặc biệt là những kiến trúc được xây dựng từ mạng neuron nhân tạo (ANN), đã từng thống trị ít nhất là tới Neocognitron được giới thiệu bởi Masahiko Fukushima vào năm 1980. Chính các ANN lại thống trị thậm chí lâu hơn nữa. Thách thức là làm thế nào để đào tạo mạng lưới này với nhiều lớp. Năm 1989, Yann Le Cun và các cộng sự đã có thể áp dụng các thuật toán truyền ngược tiêu chuẩn, khoảng từ năm 1974, đối với một mạng neuron sâu với mục đích nhận dạng chữ viết taymã ZIP trong các bức thư. Mặc dù sự thành công trong việc áp dụng thuật toán này, thời gian để đào tạo mạng dựa trên số liệu này mất khoảng 3 ngày, làm cho việc sử dụng nó vào các mục đích bình thường trở nên không thực tế. Năm 1995,Brendan Frey đã chứng minh rằng có thể đào tạo một mạng nơ ron bao gồm đầy đủ sáu lớp kết nối và vài trăm đơn vị ẩn bằng cách sử dụng thuật toán đánh thức giấc ngủ, nó được hợp tác phát triển với Peter Dayan và Geoffrey Hinton. Tuy nhiên, việc huấn luyện phải mất hai ngày.
Nhiều yếu tố góp phần vào lý do gây ra tốc độ chậm, một là vấn đề biến mất gradient được phân tích vào năm 1991 bởi Sepp Hochreiter.
Trong năm 1991 những mạng neuron như vậy được sử dụng để nhận diện chữ số viết tay 2-D cách ly, nhận dạng đối tượng 3-D được thực hiện bằng cách kết hợp các hình ảnh 2-D với một mô hình đối tượng 3-D thủ công. Juyang Weng và các cộng sự đề xuất rằng một bộ não người không sử dụng một mô hình đối tượng 3-D nguyên khối, và vào năm 1992, họ xuất bản Cresceptron, một phương pháp để thực hiện nhận dạng đối tượng 3-D trực tiếp từ các hậu trường lộn xộn. Cresceptron là một ghép tầng của các lớp tương tự như Neocognitron. Nhưng trong khi Neocognitron yêu cầu một lập trình viên con người can thiệp, Cresceptron sẽ tự động học được một số đặc điểm không có giám sát trong mỗi lớp, nơi mà mỗi đặc điểm được đại diện bởi một nhân tích chập. Cresceptron cũng phân đoạn từng đối tượng học được từ một cảnh nền lộn xộn thông qua việc phân tích ngược mạng đó. Thăm dò max, bây giờ thường được thông qua bởi các mạng neuron sâu (ví dụ: các kiểm tra ImageNet), lần đầu tiên sử dụng trong Cresceptron để giảm độ phân giải vị trí bởi của một hệ số (2x2) đến 1 thông qua việc ghép tầng tổng quát hóa tốt hơn. Mặc dù có những lợi thế như thế, các mô hình đơn giản hơn sử dụng nhiệm vụ cụ thể có đặc điểm thủ công như bộ Gabor và các máy hỗ trợ vector (SVM-support vector machines) đã là lựa chọn phổ biến trong thập niên 1990 và thập niên 2000, bởi vì chi phí tính toán bởi các ANN và vì thiếu sự hiểu biết về cách thức bộ não tự quản các kết nối mạng sinh học của nó.
Trong lịch sử lâu dài của nhận dạng giọng nói, cả học nông và học sâu (ví dụ, các mạng tái phát) của mạng neuron nhân tạo đã được khám phá trong nhiều năm. Nhưng những phương pháp này không bao giờ thắng được công nghệ mô hình hỗn hợp/mô hình Markov ẩn Gaussian (GMM-HMM) thủ công-nội bộ dựa trên các mô hình thể sinh của việc huấn luyện nhận dạng giọng nói một cách rõ ràng.
Một số khó khăn chính đã được phân tích một cách có phương pháp, bao gồm giảm bớt gradient và cấu trúc tương quan thời gian yếu và trong các mô hình tiên đoán thần kinh. Những khó khăn bổ sung đó là thiếu dữ liệu huấn luyện lớn và khả năng tính toán yếu trong thời gian ban đầu. Vì vậy, hầu hết nhà nghiên cứu nhận dạng giọng nói đã hiểu rõ các rào cản như vậy đã chuyển ra khỏi các mạng nơ ron để theo đuổi mô hình thể sinh, cho đến khi một sự hồi sinh gần đây của học sâu đã vượt qua tất cả những khó khăn này. Hinton và các cộng sự và Đặng cùng các cộng sự đã xem xét một phần của lịch sử này gầy đây về cách họ cộng tác với nhau và sau đó với các đồng nghiệp giữa các nhóm tái phát động nghiên cứu mạng neuron và bắt đầu nghiên cứu học sâu và các ứng dụng nhận dạng giọng nói.


== Các mạng neuron nhân tạo ==
Một số phương pháp học sâu thành công nhất là mạng neuron nhân tạo. Mạng neuron nhân tạo được lấy cảm hứng từ các mô hình sinh học năm 1959 được đề xuất bởi người đoạt giải Nobel David H. Hubel & Torsten Wiesel, 2 người đã tìm thấy hai loại tế bào trong vỏ não thị giác chính: các tế bào đơn giản vàcác tế bào phức tạp. Nhiều mạng neuron nhân tạo có thể được xem như là các mô hình ghép tầng của các tế bào loại lấy cảm hứng từ những quan sát sinh học.
Neocognitron của Fukushima giới thiệu các mạng neuron tích chập được đào tạo một phần bởi học không có giám sát với các đặc điểm được con người hướng dẫn trong mặt phẳng thần kinh. Yann LeCun...(1989) áp dụng truyền ngược có giám sát cho các kiến trúc như vậy. Weng... (1992) công bố các mạng neuron tích chập Cresceptron để nhậ dạng các đối tượng 3-D từ các hình ảnh có hậu trường lộn xộn và phân khúc của các đối tượng từ hình ảnh đó.
Một nhu cầu rõ ràng để nhận dạng các đối tượng 3-D nói chung là ít nhất là thay đổi tính bất biến và khả năng chịu biến dạng. Thăm dò Max (Max-pooling) xuất hiện lần đầu tiên được đề xuất bởi Cresceptron để kích hoạt mạng để chịu đựng được sự biến dạng từ nhỏ đến lớn theo một cách phân cấp, trong khi sử dụng tích chập. Thăm dò mã đã hoạt động tốt, nhưng không đảm bảo, dịch chuyển bất định ở mức điểm ảnh.
Với sự ra đời của thuật toán truyền ngược được khám phá ra một cách độc lập bởi nhiều nhóm trong thập niên 1970 và 1980, nhiều nhà nghiên cứu đã cố gắng để đào tạo các mạng neuron nhân tạo sâu có giám sát từ đầu, ban đầu với rất ít thành công. Luận văn tốt nghiệp cao đẳng của Sepp Hochreiter năm 1991 chính thức xác định lý do cho sự thất bại này là vấn đề biến mất gradient, ảnh hưởng đến các mạng nuôi tiến nhiều lớp và các mạng neuron hồi qui. Các mạng tái phát (hồi qui) được huấn luyện bằng cách trải chúng ra vào các mạng nuôi tiến rất sâu, nơi một lớp mới được tạo ra cho mỗi bước thời gian của một chuỗi đầu vào được xử lý bởi mạng này. Khi các sai số truyền từ lớp này sang lớp khác, chúng co lại theo cấp số nhân với số lượng lớp, ngăn cản điều chỉnh trọng số nơ ron, dựa trên những sai số này.
Để khắc phục vấn đề này, một số phương pháp đã được đề xuất. Một là thứ bậc đa cấp của mạng của Jürgen Schmidhuber (1992) cấp độ một được đào tạo trước tại một thời điểm bởi học không có giám sát, điều chỉnh bởi truyền ngược. Ở đây, mỗi cấp học một đại diện bị nén của các quan sát được đưa đến cấp độ tiếp theo.
Phương pháp khác là mạng bộ nhớ dài ngắn hạn (LSTM) của Hochreiter & Schmidhuber (1997). Trong năm 2009, các mạng LSTM đa chiều sâu đã chiến thắng ba cuộc thi ICDAR năm 2009 trong nhận dạng chữ viết tay, mà không có bất kỳ kiến thức sẵn có về ba ngôn ngữ để được học.
Sven Behnke vào năm 2003 dựa chỉ vào các dấu hiệu của gradient (Rprop) khi đào tạo Kim tự tháp Trừu tượng Nơ ron của mình để giải bài toán giống như tái tạo hình ảnh và định vị khuôn mặt.
Các phương pháp khác cũng sử dụng đào tạo trước không có giám sát để tạo ra một mạng nơ ron, khiến nó lần đầu tiên học được bộ dò đặc điểm nói chung là hữu ích. Sau đó mạng này được đào tạo tiếp tục bằng cách truyền ngược có giám sát để phân loại dữ liệu có dán nhãn. Mô hình sâu này của Hinton và các cộng sự (2006) liên quan đến việc học phân phối của một đại diện cao cấp bằng cách sử dụng các lớp kế tiếp của các biến tiềm ẩn nhị phân hoặc giá trị thực. nó sử dụng một máy Boltzmann hạn chế (Smolensky, 1986) để mô hình hóa mỗi lớp mới của các đặc điểm cao cấp hơn. Mỗi lớp mới đảm bảo một sự tăng trưởng trong biên thấp của kiểm tra tỷ lệ giống của dữ liệu, do đó tăng cường cho mô hình, nếu được huấn luyện đúng cách. Một khi đã đủ nhiều lớp đã được học, kiến trúc sâu có thể được sử dụng như là một mô hình thể sinh bằng cách tái tạo dữ liệu khi lấy mẫu xuống mô hình đó (một "sự vượt qua tổ tiên") từ các kích hoạt tính năng cấp đỉnh.
Hinton báo cáo rằng các mô hình của mình là trích xuất các đặc điểm hiệu quả tính theo chiều cao, cấu trúc dữ liệu.
Nhóm Google Brain do Andrew Ng và Jeff Dean đã tạo ra một mạng nơ ron học cách để nhận dạng được những khái niệm cao cấp hơn, chẳng hạn như con mèo, chỉ từ xem những hình ảnh không được dán nhãn từ các video trên YouTube.
Các phương pháp khác dựa trên sức mạnh xử lý vượt trội của các máy tính hiện đại, đặc biệt, là các GPU. Trong năm 2010, Dan Ciresan và các đồng nghiệp trong nhóm của Jürgen Schmidhuber tại Phòng thí nghiệp AI Thụy Sĩ IDSIA cho thấy rằng mặc dù "vấn đề biến mất gradient" nêu trên, thì với sức mạnh xử lý vượt trội của các GPU làm khiến cho đồng truyền ngược đơn giản trở nên khả thi đối với các mạng neuron nuôi tiến sâu với nhiều lớp. Phương pháp này tốt hơn tất cả các kỹ thuật máy học khác trong việc giải bài toán cũ nổi tiếng MNIST chữ số viết tay của Yann Le Cun và các đồng nghiệp tại NYU.
Cùng lúc đó, cuối năm 2009, học sâu đã thực hiện xâm nhập vào nhận dạng giọng nói, khi được đánh dấu bởi Hội thảo NIPS về học sâu trong nhận dạng giọng nói. Việc tăng cường hợp tác giữa các nhà nghiên cứu của Microsoft Research và đại học Toronto đã chứng minh vào giữa năm 2010 ở Redmond rằng các mạng neuron sâu giao tiếp với một mô hình Markov ẩn với các trạng thái phụ thuộc vào ngữ cảnh xác định lớp đầu ra của mạng neuron có thể giảm mạnh lỗi trong các tác vụ nhận dạng tiếng nói có vốn từ vựng lớn như tìm kiếm qua giọng nói. Cùng một mô hình mạng thần kinh sâu được chỉ ra cho quy mô lên đến các tác vụ cấp Tổng đài khoảng một năm sau đó tại Microsoft Research châu Á.
Tính đến năm 2011, tiến bộ trong các mạng nuôi tiến học sâu đã thay thế các lớp tích chập và các lớp thăm dò tối da (max-pooling), đứng đầu bởi một số lớp có đầy đủ kết nối hoặc kết nối từng phần theo sau bởi một lớp phân loại cuối cùng. Việc huấn luyện thường được thực hiện mà không có bất kỳ đào tạo trước không có giám sát nào. Từ năm 2011, các thực thi dựa trên GPU của hướng tiếp cận này đã thắng nhiều cuộc thi nhận dạng hình mẫu, bao gồm cuộc thi IJCNN 2011 Traffic Sign Recognition Competition, ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, và các cuộc thi khác.
Các phương pháp học sâu có giám sát như vậy cũng đã là bộ nhậng dạng mô hình nhân tạo đầu tiên đạt được hiệu suất có thể cạnh tranh lại được với con người trong những công việc nhất định.
Để vượt qua những rào cản của AI yếu được đại diện bằng học sâu, cần phải để vượt qua các kiến trúc học sâu, bởi vì bộ não sinh học sử dụng cả mạch học nông và học sâu theo báo cáo của ngành giải phẫu não bộ chỉ ra một loạt các tính bất biến. Weng lập luận rằng não tự kết nối chủ yếu theo các thống kê tín hiệu và, do đó, một phân tầng nối tiếp không thể bắt tất cả các vật phụ thuộc thống kê chủ yếu. Các ANN đã có thể đảm bảo sự thay đổi bất biến để đối phó với các đối tượng tự nhiên lớn và nhỏ trong hậu trường có sự xáo trộn lớn, chỉ khi các bất định mở rộng vượt ra ngoài sự thay đổi, tới tất cả các khái niệm ANN đã học được, chẳng hạn như vị trí, loại (nhãn lớp đối tượng), quy mô, ánh sáng. Điều này được thực hiện trong các Mạng Phát triển (DN) có biểu hiện là Where-What Networks, WWN-1 (2008) cho đến WWN-7 (2013).


== Kiến trúc ==
Có một lượng rất lớn các biến thể của kiến trúc sâu. Hầu hết chúng là nhánh sinh ra từ một số kiến trúc cha ban đầu. Không phải là luôn luôn có thể so sánh hiệu suất của nhiều kiến trúc cùng với nhau, vì chúng không phải là tất cả đánh giá trên cùng một tập dữ liệu. Học sâu học là một lĩnh vực phát triển nhanh, và các kiến trúc, biến thể, hoặc các thuật toán mới xuất hiện mỗi vài tuần.


=== Các mạng neuron sâu ===
Mạng neuron sâu (DNN-Deep neural Network) là một mạng neuron nhân tạo (ANN) với nhiều đơn vị lớp ẩn giữa lớp đầu vào và đầu ra. Tương tự như các ANN nông, các DNN nông có thể mô hình mối quan hệ phi tuyến phức tạp. Các kiến trúc DNN, ví dụ như để phát hiện và phân tích đối tượng tạo ra các mô hình hỗn hợp trong đó đối tượng này được thể hiện như một thành phần được xếp lớp của các hình ảnh nguyên thủy. Các lớp phụ cho phép các thành phần của các đặc điểm từ các lớp thấp hơn, đem lại tiềm năng của mô hình hóa dữ liệu phức tạp với các đơn vị ít hơn so với một mạng lưới nông thực hiện tương tự như vậy.
Các DNN thường được thiết kế như các mạng nuôi tiến, nhưng nghiên cứu gần đây đã áp dụng thành công kiến trúc học sâu đối với các mạng nơ ron tái phát cho các ứng dụng chẳng hạn như mô hình hóa ngôn ngữ. Các mạng neuron sâu tích chập (CNN) được sử dụng trong thị giác máy tính nơi thành công của chúng đã được ghi nhận. Gần đây hơn, các CNN đã được áp dụng để mô hình hóa âm thanh cho nhận dạng giọng nói tự động (ASR), nơi chúng đã cho thấy sự thành công trong các mô hình trước đó. Để đơn giản, ta hãy nhìn vào việc huấn luyện các DNN được đưa ra ở đây.
Một DNN có thể là mô hình biết suy xét được đào tạo với thuật toán truyền ngược tiêu chuẩn. Các bản cập nhật trọng số có thể được thực hiện thông qua độ dốc gradient ngẫu nhiên bằng cách sử dụng phương trình sau:
Trong đó, 
  
    
      
        η
      
    
    {\displaystyle \eta }
  
 là tốc độ học, và 
  
    
      
        C
      
    
    {\displaystyle C}
  
 là hàm chi phí. Việc lựa chọn của hàm chi phí phụ thuộc vào các yếu tố như loại học tập (giám sát, không có giám sát, tăng cường, vv) và hàm kích hoạt. Ví dụ, khi thực hiện học có giám sát về một vấn đề phân loại nhiều lớp, các lựa chọn phổ biến cho hàm kích hoạt và hàm chi phí là hàm softmax (hàm mũ chuẩn hóa) và hàm entropy chéo, tương ứng. Hàm softmax được định nghĩa là 
  
    
      
        
          p
          
            j
          
        
        =
        
          
            
              exp
              ⁡
              (
              
                x
                
                  j
                
              
              )
            
            
              
                ∑
                
                  k
                
              
              exp
              ⁡
              (
              
                x
                
                  k
                
              
              )
            
          
        
      
    
    {\displaystyle p_{j}={\frac {\exp(x_{j})}{\sum _{k}\exp(x_{k})}}}
  
 trong đó 
  
    
      
        
          p
          
            j
          
        
      
    
    {\displaystyle p_{j}}
  
 thể hiện xác suất của lớp (đầu ra của đơn vị 
  
    
      
        j
      
    
    {\displaystyle j}
  
) và 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
  
 và 
  
    
      
        
          x
          
            k
          
        
      
    
    {\displaystyle x_{k}}
  
 hiển thể hiện tổng đầu vào thành các đơn vị 
  
    
      
        j
      
    
    {\displaystyle j}
  
 và 
  
    
      
        k
      
    
    {\displaystyle k}
  
 của cùng cấp tương ứng. Entropy chéo được định nghĩa là 
  
    
      
        C
        =
        −
        
          ∑
          
            j
          
        
        
          d
          
            j
          
        
        log
        ⁡
        (
        
          p
          
            j
          
        
        )
      
    
    {\displaystyle C=-\sum _{j}d_{j}\log(p_{j})}
  
 trong đó 
  
    
      
        
          d
          
            j
          
        
      
    
    {\displaystyle d_{j}}
  
 thể hiện cho xác suất mục tiêu của đơn vị ra 
  
    
      
        j
      
    
    {\displaystyle j}
  
 và 
  
    
      
        
          p
          
            j
          
        
      
    
    {\displaystyle p_{j}}
  
 là đầu ra xác suất cho 
  
    
      
        j
      
    
    {\displaystyle j}
  
 sau khi áp dụng hàm kích hoạt.
Chúng có thể được sử dụng để xuất ra các hộp bao quanh đối tượng trong hình thức của một mặt nạ nhị phân. Chúng cũng được sử dụng cho các hồi quy đa quy mô để tăng độ chính xác của định vị. Hồi qui dựa trên DNN có thể học các đặc điểm mà chụp lại thông tin hình học ngoài việc là một bộ phân loại tốt. Chúng sẽ loại bỏ các giới hạn của việc thiết kế một mô hình mà sẽ chụp lại các bộ phận và quan hệ của chúng một cách rõ ràng. Điều này sẽ giúp học được một loạt các đối tượng rộng lớn. Mô hình này bao gồm nhiều lớp, mỗi trong số đó có một đơn vị chỉnh lại tuyến tính cho các chuyển đổi phi tuyến. Một số lớp là tích chập, trong khi những lớp khác được kết nối đầy đủ. Mỗi lớp tích chập có một thăm dò max bổ sung. Mạng được huấn luyện để giảm thiểu sai số L2 để dự đoán mặt nạ nằm trong dãi qua bộ huấn luyện toàn bộ chứa các hộp đường biên được thể hiện như là mặt nạ.


==== Các vấn đề với các mạng neuron sâu ====
Như với các ANN, nhiều vấn đề có thể nảy sinh với các DNN nếu chúng được huấn luyện thô sơ. Hai vấn đề phổ biến là overfitting (nhiễu hoặc sai số ngẫu nhiên) và thời gian tính toán.
Các DNN có thiên hướng overfitting vì được thêm các lớp trừu tượng, mà cho phép chúng thực hiện mô hình hóa phụ thuộc hiếm hoi vào dữ liệu huấn luyện. Các phương pháp regularization (quy tắc hóa) như phân rã trọng số (
  
    
      
        
          ℓ
          
            2
          
        
      
    
    {\displaystyle \ell _{2}}
  
-regularization) hoặc sparsity (rãi) (
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell _{1}}
  
-regularization) có thể được áp dụng trong quá trình huấn luyện để giúp chống lại overfitting. Một phương pháp regularization gần đây được áp dụng cho các DNN là dropout regularization. Trong dropout, một số số lượng đơn vị được bỏ qua ngẫu nhiên từ các lớp ẩn trong quá trình đào tạo. Điều này giúp phá vỡ các phụ thuộc hiếm hoi có thể xảy ra trong dữ liệu đào tạo.
Phương pháp chủ đạo cho việc huấn luyện các cấu trúc là sửa lỗi huấn luyện (chẳng hạn như truyền ngược với gradient descent) do dễ thực hiện và xu hướng hội tụ tốt hơn local optima (tối hưu cục bộ) hơn so với các phương pháp huấn luyện khác. Tuy nhiên, những phương pháp này có thể tốn công tính toán hơn, đặc biệt là cho các DNN. Có rất nhiều tham số huấn luyện để được xem xét với một DNN, chẳng hạn như kích thước (số lượng lớp và số lượng đơn vị trên mỗi lớp), tốc độ học và trọng số ban đầu. Quét thông qua không gian tham số cho các thông số tối ưu có thể không khả thi do chi phí trong thời gian và tài nguyên tính toán. Nhiều 'mẹo vặt' chẳng hạn như bằng cách sử dụng mini-batching (tính toán gradient trên nhiều ví dụ huấn luyện khác nhau cùng một lúc chứ không phải là từng ví dụ một) đã được chỉ ra để tăng tốc độ tính toán. Lượng xử lý lớn thông qua GPU đã tăng tốc đáng kể trong việc huấn luyện, do tính toán ma trận và vector rất thích hợp với các GPU. Lựa chọn thay thế triệt để cho truyền ngược là Extreme Learning Machines (Siêu máy học, các mạng "No-prop", huấn luyện không cần truy ngược, các mạng "không trọng số", và mạng nơron không kết (non-connectionist neural network) đang thu hút được sự chú ý.


=== Mạng niềm tin sâu (Deep belief network) ===
Một mạng niềm tin sâu (DBN) là một mô hình xác suất thể sinh, tạo thành bởi nhiều đơn vị ẩn nhiều lớp. Nó có thể được coi là một hàm hợp các mô-đun học đơn giản tạo thành mỗi lớp.
Một DBN có thể được sử dụng để huấn luyện trước khả sinh một DNN bằng cách sử dụng các trọng số DBN học như các trọng số DNN ban đầu. Các thuật toán truyền ngược hoặc suy xét khác sau đó có thể được áp dụng để điều chỉnh những trọng số này. Điều này đặc biệt hữu ích khi dữ liệu đào tạo giới hạn là có sẵn, vì các trọng số khởi tạo nghèo nàn có thể cản trở đáng kể hiệu suất của mô hình được học. Các trọng số đào tạo trước này là một vùng không gian trọng số là gần gũi hơn với trọng số tối ưu hơn là các trọng số ban đầu được chọn ngẫu nhiên. Điều này cho phép cả mô hình hóa được cải thiện và hội tụ tinh chỉnh pha nhanh hơn.
Một DBN có thể được huấn luyện một cách hiệu quả trong một cách thức không có giám sát, lớp kề lớp, nơi mà các lớp thường được tạo ra từ các máy Boltzmann hạn chế(RBM). Một RBM là một mô hình vô hướng, thể sinh dựa trên năng lượng với một lớp đầu vào "hiện" và một ẩn lớp, và các kết nối giữa các lớp nhưng không nằm trong các lớp. Phương pháp huấn luyện cho RBM được đề xuất bởi Geoffrey Hinton để sử dụng với các mô hình "Product of Expert" được gọi là tương phản phân kỳ (CD-contrastive divergence). CD cung cấp một xấp xỉ cho phương pháp với khả năng tối đa có vị trí lý tưởng sẽ được áp dụng cho việc học các trọng số của RBM. Trong việc huấn luyện một RBM đơn, các cập nhật trọng số được thực hiện với gradient ascent qua phương trình sau:
  
    
      
        Δ
        
          w
          
            i
            j
          
        
        (
        t
        +
        1
        )
        =
        
          w
          
            i
            j
          
        
        (
        t
        )
        +
        η
        
          
            
              ∂
              log
              ⁡
              (
              p
              (
              v
              )
              )
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
      
    
    {\displaystyle \Delta w_{ij}(t+1)=w_{ij}(t)+\eta {\frac {\partial \log(p(v))}{\partial w_{ij}}}}
  
. Trong đó, 
  
    
      
        p
        (
        v
        )
      
    
    {\displaystyle p(v)}
  
 là xác suất của một vector hiện, được cho bởi 
  
    
      
        p
        (
        v
        )
        =
        
          
            1
            Z
          
        
        
          ∑
          
            h
          
        
        
          e
          
            −
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle p(v)={\frac {1}{Z}}\sum _{h}e^{-E(v,h)}}
  
. 
  
    
      
        Z
      
    
    {\displaystyle Z}
  
 là hàm từng phần, (được sử dụng để chuẩn hóa) và 
  
    
      
        E
        (
        v
        ,
        h
        )
      
    
    {\displaystyle E(v,h)}
  
 là hàm năng lượng được gán cho trạng thái của mạng. Một năng lượng thấp hơn chỉ thị mạng đó đang được cấu hình "đáng mong muốn" hơn. Gradient 
  
    
      
        
          
            
              ∂
              log
              ⁡
              (
              p
              (
              v
              )
              )
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial \log(p(v))}{\partial w_{ij}}}}
  
 có dạng đơn giản 
  
    
      
        ⟨
        
          v
          
            i
          
        
        
          h
          
            j
          
        
        
          ⟩
          
            data
          
        
        −
        ⟨
        
          v
          
            i
          
        
        
          h
          
            j
          
        
        
          ⟩
          
            model
          
        
      
    
    {\displaystyle \langle v_{i}h_{j}\rangle _{\text{data}}-\langle v_{i}h_{j}\rangle _{\text{model}}}
  
 trong đó 
  
    
      
        ⟨
        ⋯
        
          ⟩
          
            p
          
        
      
    
    {\displaystyle \langle \cdots \rangle _{p}}
  
 thể hiện các giá trị trung bình đối với phân phối 
  
    
      
        p
      
    
    {\displaystyle p}
  
. Vấn đề này nãy sinh trong việc lấy mẫu 
  
    
      
        ⟨
        
          v
          
            i
          
        
        
          h
          
            j
          
        
        
          ⟩
          
            model
          
        
      
    
    {\displaystyle \langle v_{i}h_{j}\rangle _{\text{model}}}
  
 bởi vì điều này đòi hỏi phải chạy xen kẽ lấy mẫu Gibbs trong một thời gian dài. CD thay thế bwowcs này bằng cách chạy luân phiên lấy mẫu Gibbs cho 
  
    
      
        n
      
    
    {\displaystyle n}
  
 bước (giá trị của 
  
    
      
        n
        =
        1
      
    
    {\displaystyle n=1}
  
 được lấy theo kinh nghiệm được chỉ ra là làm việc tốt). Sau 
  
    
      
        n
      
    
    {\displaystyle n}
  
 bước, dữ liệu được lấy mẫu và mẫu này sẽ được sử dụng trong 
  
    
      
        ⟨
        
          v
          
            i
          
        
        
          h
          
            j
          
        
        
          ⟩
          
            model
          
        
      
    
    {\displaystyle \langle v_{i}h_{j}\rangle _{\text{model}}}
  
. Chu trình CD hoạt động như sau:

Khởi tạo các đơn vị hiện (visible) tới một vector huấn luyện.
Cập nhật các đơn vị ẩn song song với các đơn vị hiện: 
  
    
      
        p
        (
        
          h
          
            j
          
        
        =
        1
        ∣
        
          
            V
          
        
        )
        =
        σ
        (
        
          b
          
            j
          
        
        +
        
          ∑
          
            i
          
        
        
          v
          
            i
          
        
        
          w
          
            i
            j
          
        
        )
      
    
    {\displaystyle p(h_{j}=1\mid {\textbf {V}})=\sigma (b_{j}+\sum _{i}v_{i}w_{ij})}
  
. 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
 là hàm sigmoid và 
  
    
      
        
          b
          
            j
          
        
      
    
    {\displaystyle b_{j}}
  
 là độ lệch của 
  
    
      
        
          h
          
            j
          
        
      
    
    {\displaystyle h_{j}}
  
.
Cập nhật các đơn vị hiện song song với các đơn vị ẩn đã cho: 
  
    
      
        p
        (
        
          v
          
            i
          
        
        =
        1
        ∣
        
          
            H
          
        
        )
        =
        σ
        (
        
          a
          
            i
          
        
        +
        
          ∑
          
            j
          
        
        
          h
          
            j
          
        
        
          w
          
            i
            j
          
        
        )
      
    
    {\displaystyle p(v_{i}=1\mid {\textbf {H}})=\sigma (a_{i}+\sum _{j}h_{j}w_{ij})}
  
. 
  
    
      
        
          a
          
            i
          
        
      
    
    {\displaystyle a_{i}}
  
 là độ lệch của 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  
. Điều này được gọi là bước "cải tạo".
Tái cập nhật các đơn vị ẩn song song với các đơn vị hiện cải tạo đã cho bằng cách sử dụng phương trình tương tự như trong bước 2.
Thực hiện cập nhật trọng số: 
  
    
      
        Δ
        
          w
          
            i
            j
          
        
        ∝
        ⟨
        
          v
          
            i
          
        
        
          h
          
            j
          
        
        
          ⟩
          
            data
          
        
        −
        ⟨
        
          v
          
            i
          
        
        
          h
          
            j
          
        
        
          ⟩
          
            reconstruction
          
        
      
    
    {\displaystyle \Delta w_{ij}\propto \langle v_{i}h_{j}\rangle _{\text{data}}-\langle v_{i}h_{j}\rangle _{\text{reconstruction}}}
  
.
Khi một RBM được huấn luyện, RBM khác là "xếp chồng" trên nó, đưa đầu vào của nó từ cuối lớp đã được huấn luyện. Lớp hiện mới này được khởi tạo với một vector hiện, và các giá trị cho các đơn vị trong các lớp đã được huấn luyện phân công bằng cách sử dụng trọng số hiện tại và các độ lệch. RBM mới này sau đó lại được huấn luyện với chu trình như trên. Toàn bộ quá trình này được lặp lại cho đến khi một số tiêu chí mong muốn chặn lại được đáp ứng.
Mặc dù xấp xỉ của CD để tối đa khả năng là rất thô (CD đã được chỉ ra là theo gradient của bất kỳ hàm nào), nó đã được kinh nghiệm chỉ ra là có hiệu quả trong huấn luyện các kiến trúc sâu.


=== Mạng nơ ron tích chập (Convolutional neural networks) ===
Một CNN gồm có một hoặc nhiều hơn các lớp tích chập với các lớp đầy đủ kết nối (đáp ứng phù hợp với những mạng neuron nhân tạo tiêu biểu) trên đỉnh. Nó cũng sử dụng trọng số gắn liền và các lớp thăm dò. Kiến trúc này cho phép các CNN tận dụng lợi thế của cấu trúc 2D của dữ liệu đầu vào. So với những kiến trúc sâu khác, mạng neuron tích chập đang bắt đầu thể hiện kết quả vượt trội trong các ứng dụng hình ảnh và giọng nói. Chúng cũng có thể được huấn luyện với tiêu chuẩn truyền ngược. CNN dễ dàng được đào tạo hơn các mạng nơ ron sâu nuôi tiến thông thường khác, và có ít thông số ước tính hơn, khiến cho chúng trở thành một kiến trúc rất hấp dẫn để sử dụng. Các ví dụ về ứng dụng trong Thị Giác máy tính bao gồm DeepDream.


=== Các mạng niềm tin sâu tích chập ===
Sử dụng mạng niềm tin sâu (CDBN) là một thành tựu gần đây của học sâu. Các CDBN có cấu trúc rất giống với một mạng neuron tích chập và được huấn luyện tương tự như các mạng niềm tin sâu. Vì vậy, chúng khai thác cấu trúc 2D của hình ảnh, giống như CNN làm, và làm cho việc sử dụng đào tạo trước giống như mạng niềm tin sâu. Chúng quy định một cấu trúc chung mà có thể được sử dụng trong nhiều tác vụ xử lý hình ảnh và tín hiệu. Gần đây, nhiều kết quả benchmark (tiêu chuẩn) dựa trên tập dữ liệu hình ảnh chuẩn như CIFAR đã được thu được kết quả bằng cách sử dụng CDBN.


=== Mạng neuron lưu trữ và truy xuất bộ nhớ lớn ===
Mạng nơ ron lưu trữ và truy xuất bộ nhớ lớn (LAMSTAR) là các mạng nơ ron học sâu nhanh gồm nhiều lớp mà có thể sử dụng đồng thời nhiều bộ lọc. Các bộ lọc này có thể là phi tuyến, ngẫu nhiên, logic, không cố định, hoặc thậm chí không có tính phân tích. Chúng là học sinh học năng động và liên tục.
Mạng neuron LAMSTAR có thể phục vụ như là một mạng nơ ron năng động trong không gian hay miền thời gian, hoặc cả hai. Tốc độ của nó được quy định bởi các liên kết-trọng số Hebbian (chương 9 của D. Graupe, 2013), dùng để tích hợp các bộ lọc khác nhau và thường khác nhau (các hàm tiền xử lý) vào nó nhiều lớp và để xếp hạng năng đọng tầm quan trọng của các lớp khác nhau và các hàm liên quan đến nhiệm vụ nhất định cho việc học sâu. Điều này hiển nhiên bắt chước học sinh học mà tích hợp các bộ tiền lý đầu ra khác nhau (ốc tai, võng mạc, vv) và vỏ não (thính giác, thị giác, vv) và của các vùng khác nhau của chúng. Khả năng học sâu của nó tăng cường hơn nữa bằng cách sử dụng sự ức chế, sự tương quan và bởi khả năng đối phó với dữ liệu không đầy đủ của nó, hoặc "mất" nơ ron hoặc lớp ngay cả khi đang thực thi một tác vụ. Hơn nữa, nó hoàn toàn minh bạch do trọng số liên kết của nó. Các trọng số liên kết cho phép xác định năng động sáng tạo và thừa thải, và tạo thuận lợi cho việc xếp hạng của các lớp, các bộ lọc hoặc các nơ ron đơn lẽ tương ứng với một nhiệm vụ.
LAMSTAR đã được áp dụng cho nhiều dự đoán y tế và tài chính (xem Graupe, 2013 Phần 9C), bộ lọc thích nghi nhiễu nhận dạng giọng nói với tiếng ồn không xác định, nhận dạng ảnh tĩnh (Graupe, 2013 Phần 9D), nhận dạng ảnh video, bảo mật phần mềm, điều khiển thích nghi của các hệ thống phi tuyến, vv. LAMSTAR có tốc độ tính toán nhanh hơn nhiều và có lỗi hơi ít hơn so với một mạng nơ ron tích chập dựa trên các bộ lọc hàm-ReLU và thăm dò max, trong một nghiên cứu nhận dạng ký tự so sánh.
Các ứng dụng này chứng minh đào sâu vào các khía cạnh của các dữ liệu đó là bị ẩn từ các mạng học nông hoặc thậm chí từ những giác quan của con người (mắt, tai), chẳng hạn như trong trường hợp của dự đoán sự bắt đầu của hiện tượng ngưng thở khi ngủ, của một biểu đồ điện tâm đồ một thai nhi như được ghi chép từ các điện cực gắn trên da được đặt trên bụng người mẹ trong thời gian đầu của thai kỳ, của dự đoán tài chính (Phần 9C trong Graupe, 2013), hoặc trong lọc mù của nhiễu trong nhận dạng giọng nói
LAMSTAR đã được đề xuất năm 1996 (Bằng phát minh 5,920,852 A của Mỹ) và tiếp tục được phát triển bởi D Graupe và H Kordylewski vào năm 1997-2002. Một phiên bản sửa đổi, được gọi là LAMSTAR 2, được phát triển bởi N C Schneider và D Graupe trong năm 2008.


=== Các máy Deep Boltzmann ===


=== Các mạng xếp chồng sâu ===
Một kiến trúc sâu dựa trên một hệ thống phân cấp của các khối mô-đun mạng neuron đơn giản là một mạng sâu lồi, được giới thiệu vào năm 2011. Ở đây, bài toán học các trọng số được xây dựng như một bài toán tối ưu hóa lồi với lời giải dạng đóng. Kiến trúc này còn được gọi là một mạng xếp chồng sâu (DSN), nhấn mạnh các cơ chế tương tự với tổng quát hóa xếp chồng. Mỗi khối DSN là một module đơn giản đó là dễ dàng để huấn luyện chính nó trong một kiểu có giám sát mà không cần truyền ngược cho toàn bộ các khối.


=== Mạng lập trình sâu (deep coding network) ===
Có những lợi thế của một mô hình mà có thể chủ động cập nhật bản thân từ ngữ cảnh trong dữ liệu. Mạng lập trình (DPCN) là một chương trình lập trình tiên đoán, trong đó thông tin từ trên xuống được sử dụng để điều chỉnh theo kinh nghiệm của những cái trước đó cần thiết cho một thủ tục suy luận từ dưới lên bằng các phương tiện của một mô hình thể sinh kết nối cục bộ sâu. Điều này hoạt động bằng cách chiết tách các đặc điểm rời rạc các quan sát biến đổi theo thời gian bằng cách sử dụng một mô hình động học tuyến tính. Sau đó, một chiến lược thăm dò được sử dụng để học các đại diện đặc điểm bất biến. Các đơn vị này tập hợp lại để tạo thành một kiến trúc sâu và được huấn luyện bởi học không giám sát layer-wise tham lam. Các lớp tạo thành một loại xích Markov mà các trạng thái tại bất kỳ lớp nào cũng chỉ phụ thuộc vào các lớp trước và các lớp sau (kế thừa).
Mạng lập tình dự đoán sâu (DPCN) dự đoán đại diện của lớp, bằng cách sử dụng một cách tiếp cận từ trên xuống bằng cách sử dụng thông tin ở lớp trên và các phụ thuộc thời gian từ các trạng thái trước đó.
DPCN có thể được mở rộng để tạo thành một mạng tích chập.


=== Máy nhân đa lớp ===


=== Deep q-networks ===


=== Mạng bộ nhớ ===
Bộ nhớ ngoài tích hợp với các mạng neuron nhân tạo tính đến nghiên cứu đầu tiên trrong đại diện phân phối và các bản đồ tự tổ chức. Ví dụ, trong bộ nhớ phân tán hoặc bộ nhớ phân cấp thời gian, các mô hình được mã hóa bởi các mạng neuron được sử dụng như là các địa chỉ cho bộ nhớ có khả năng định địa chỉ nội dung, với các "nơ ron" chủ yếu phục vụ như là các bộ mã hóa và giải mã.


==== Bộ nhớ ngắn-hạn dài ====
Trong thập niên 1990 và thập niên 2000, đã có nhiều công trình liên quan đến bộ nhớ ngắn-hạn dài (LSTM - thêm bộ nhớ khả vi cho các hàm hồi qui). Ví dụ:

Các hành động đẩy và lấy ra khả vi cho các mạng bộ nhớ thay thế được gọi là các máy ngăn xếp nơ ron
Memory networks where the control network's external differentiable storage is in the fast weights of another network
LSTM "forget gates"
Self-referential recurrent neural networks (RNNs) with special output units for addressing and rapidly manipulating each of the RNN's own weights in differentiable fashion (internal storage)
Learning to transduce with unbounded memory


==== Các mạng bộ nhớ ====
Các mạng bộ nhớ một mở rộng khác của các mạng nơ ron nhân tạo kết hợp với bộ nhớ dài hạn, được phát triển bởi nhóm nghiên cứu Facebook. Bộ nhớ dài hạn có thể được đọc và ghi vào đó, với mục đích sử dụng cho việc dự báo. Các mô hình này đã được áp dụng trong bối cảnh hỏi đáp (QA) nơi bộ nhớ dài hạn hoạt động hiệu quả như một cơ sở kiến thức (năng động), và đầu ra là một đáp ứng văn bản.


==== Các mạng mã hóa-giải mã ====
Một framework mã hóa-giải mã là một framework dựa trên các mạng neuron nhằm mục đích lập bản đồ đầu vào cấu trúc cao tới đầu ra có cấu trúc cao. Nó đã được đề xuất gần đây trong bối cảnh của máy dịch, trong đó đầu vào và đầu ra được viết thành câu bằng hai ngôn ngữ tự nhiên. Trong đó, một mạng nơ ron tái phát (RNN) hoặc mạng neuron tích chập (CNN) được sử dụng như một bộ mã hóa để tóm tắt một câu nguồn và tóm tắt này được giải mã bằng cách sử dụng một mô hình ngôn ngữ mạng neuron tái phát có điều kiện để tạo ra bản dịch. Tất cả các hệ thống này có các khối xây dựng tương tự: cổng RNN và CNN, và các cơ chế tập trung được huấn luyện.


== Ứng dụng ==


=== Xử lý ngôn ngữ tự nhiên (Nature Language Processing) ===
Hiện nay các mô hình transformer base đã vượt xa các loại mô hình sử dụng RNN. Hầu như trong tất cả các tác vụ Transformer base model đều vượt trội hơn các RNN model (LSTM or GRU base). Với Hugging face Hugging Face Hub chúng ta có thể dễ dàng fine turn model🤗.
Một trong những nguyên tắc cơ bản của học sâu là để thoát khỏi kỹ thuật đặc tính thủ công và sử dụng các đặc tính thô. Nguyên tắc này được khám phá thành công đầu tiên trong kiến trúc của tự mã hóa sâu trên ảnh phổ "thô" hoặc các đặc điểm dãi lọc tuyến tính, hiển thị sự vượt trội của nó hơn các tính năng Mel-Cepstral mà có chứa một vài giai đoạn chuyển đổi cố định từ ảnh phổ. Các tính năng thực sự "thô" của tiếng nói, dạng sóng, gần đây đã được chỉ ra để tạo ra các kết quả nhận dạng giọng nói tuyệt vời ở quy mô lớn.
Kể từ khi ra mắt thành công ban đầu của DNN cho nhận dạng tiếng nói khoảng 2009-2011, tiến độ (và hướng đi trong tương lai) có thể được tóm tắt vào 8 lĩnh vực chính:

Mở rộng quy mô lên/ra và tăng tốc quá trình đào tạo và giải mã DNN;
Huấn luyện suy luận có trình tự cho các DNN;
Xử lý đặc điểm bởi các mô hình sâu với sự hiểu biết vững chắc các cơ chế tiềm ẩn;
Thích nghi của các DNN và các mô hình sâu có liên quan;
Học đa tác vụ và học có chuyển giao bởi các DNN và các mô hình sâu liên quan; Các mạng neuron tích chập và làm thế nào để thiết kế chúng để khai thác tốt nhất kiến thức miền của giọng nói;
Mạng neuron tái phát và các biến thể giàu LSTM;
Các loại mô hình sâu bao gồm các mô hình dựa trên tensor và các mô hình tích hợp sâu thể sinh/suy xét.
Trường hợp nhận dạng tiếng nói tự động quy mô lớn lần đầu tiên và thuyết phục nhất thành công của học sâu trong lịch sử gần đây, chấp nhận bở cả công nghiệp và hàn lâm trong tất cả các lĩnh vực. Từ năm 2010 đến năm 2014, hai hội nghị lớn về xử lý tín hiệu và nhận dạng giọng nói, IEEE-ICASSP và Interspeech, đã thấy một sự gia tăng lớn các báo cáo được chấp nhận trong các báo cáo hội nghị thường niên tương ứng về chủ đề học sâu trong nhận dạng giọng nói. Quan trọng hơn, tất cả các hệ thống nhận dạng giọng nói thương mại chính (ví dụ: Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu và iFlyTek tìm kiếm bằng giọng nói và một loạt các sản phẩm của Nuance speech, vv) được dựa trên phương pháp học sâu. Xem thêm các cuộc phỏng vấn trên phương tiện truyền thông với CTO của Nuance Communications.
Thành công lây lan rộng trong nhận dạng tiếng nói đã đạt được vào năm 2011 được kế tiếp liền sau đó là nhận dạng hình ảnh ở quy mô lớn.


=== Nhận dạng hình ảnh ===
Một tập đánh giá phổ biến cho phân loại hình ảnh là tập hợp dữ liệu cơ sở dữ liệu MNIST. MNIST bao gồm các chữ số viết tay và bao gồm 60000 ví dụ huấn luyện và 10000 ví dụ kiểm tra. Như TIMIT, kích thước nhỏ của nó cho phép nhiều cấu hình được kiểm tra. Một danh sách đầy đủ các kết quả trên tập này có thể được tìm thấy trong. Kết quả tốt nhất hiện nay trên MNIST là tỷ lệ lỗi 0,23%, đạt được bởi Ciresan và các cộng sự vào năm 2012.
Tác động thực sự của học sâu trong nhận dạng hình ảnh hoặc đối tượng, một chi chính của thị giác máy tính, đã cảm thấy được vào mùa thu năm 2012 sau khi đội của Geoff Hinton và sinh viên của ông thắng trong cuộc thi quy mô lớn ImageNet bởi một biên độ đáng kể bằng phương pháp máy học nông tiên tiến nhất. Công nghệ này dựa trên các mạng tích chập sâu 20 tuổi, nhưng với quy mô lớn hơn nhiều trên một nhiệm vụ lớn hơn nhiều, vì nó đã học được rằng học sâu làm việc tốt đối nhận dạng giọng nói quy mô lớn. Trong năm 2013 và 2014, tỷ lệ lỗi trong tác vụ của ImageNet bằng cách sử dụng học sâu tiếp tục giảm xuống nhanh chóng, theo một xu hướng tương tự trong nhận dạng giọng nói quy mô lớn.
Khi tham vọng này di chuyển từ nhận dạng giọng nói tự động sang các bản dịch giọng nói tự động và hiểu được, phân loại hình ảnh gần đây đã được mở rộng với nhiệm vụ khó khăn hơn đó là tạo phụ đề cho hình ảnh tự động, trong đó có học sâu là công nghệ cơ bản thiết yếu.
Một ứng dụng ví dụ là một máy tính xe hơi cho biết được đào tạo bằng học sâu, có thể cho phép xe diễn giải các hình ảnh 360° từ camera. Một ví dụ khác là công nghệ được gọi là Facial Dysmorphology Novel Analysis (FDNA) -(Phân tích các dị tật của khuôn mặt) sử dụng để phân tích các trường hợp dị dạng của con người kết nối với cơ sở dữ liệu lớn của các hội chứng di truyền.


=== Xử lý ngôn ngữ tự nhiên ===
Mạng neuron đã được sử dụng cho việc thực hiện các mô hình ngôn ngữ kể từ đầu những năm 2000. Các kỹ thuật quan trọng trong lĩnh vực này là lấy mẫu âm và nhúng từ (word embedding). Nhúng chữ, chẳng hạn như word2vec, có thể được dùng như một lớp đại diện trong một kiến trúc học sâu, điều này sẽ biến đổi một từ đơn thành một đại diện vị trí của từ đó liên quan đến các từ khác trong bộ dữ liệu; vị trí được đại diện như là một điểm trong một không gian vector. Sử dụng một từ nhúng như là một lớp đầu vào với một mạng lưới thần kinh đệ quy (RNN-recursive neuron network) cho phép đào tạo mạng để phân tích cú pháp câu và cụm từ bằng cách sử dụng một ngữ pháp vector tổng hợp có hiệu quả. Một ngữ pháp vector tổng hợp có thể được coi làngữ pháp không phụ thuộc ngữ cảnh xác suất (PCFG-probabilistic context free grammar) được thực hiện bởi một mạng thần kinh đệ quy. Tự động-mã hóa đệ qui được xây dựng trên đỉnh từ nhúng đã được đào tạo để đánh giá câu tương tự và phát hiện các chú giải dài dòng. Các kiến trúc thần kinh sâu đã đạt được những kết quả tiên tiến nhất trong nhiều tác vụ xử lý ngôn ngữ tự nhiên như phân tích thống kê, phân tích tình cảm, tra cứu thông tin, dịch máy, liên kết thực thể ngữ cảnh, và.v.v.


=== Khám phá dược phẩm và độc chất học ===
Ngành công nghiệp dược phẩm phải đối mặt với vấn đề mà một tỷ lệ lớn các loại thuốc tiềm năng thất bại khi tiếp cận với thị trường. Những thất bại của các hợp chất hóa học này gây ra bởi không đủ hiệu quả trên mục tiêu phân tử sinh học (có hiệu lực với mục tiêu), có các tương tác không bị phát hiện và không mong muốn với các phân tử sinh học khác (chệch mục tiêu tác động), hoặc các hiệu ứng độc dược ngoài dự tính. Trong năm 2012, một nhóm dẫn đầu bởi George Dahl đã chiến thắng "Merck Molecular Activity Challenge" sử dụng các mạng neuron sâu đa tác vụ để dự đoán mục tiêu phân tử sinh học của một hợp chất. Trong năm 2014, nhóm của Sepp Hochreiter sử dụng học sâu để phát hiện ra mục tiêu lạ và các ảnh hưởng độc dược của các môi trường hóa chất trong các chất dinh dưỡng, sản phẩm gia dụng và thuốc men và đã chiến thắng "Tox21 Data Challenge" của NIH, FDA và NCATS. Những thành công ấn tượng chỉ ra rằng học sâu có thể vượt trội so với các phương pháp kiểm tra ảo khác. Các nhà nghiên cứu đến từ Google và Stanford đã mở rộng học sâu để khám phá dược phẩm bằng cách kết hợp dữ liệu từ nhiều nguồn khác nhau. Năm 2015, Atomwise giới thiệu AtomNet, mạng neuron học sâu đầu tiên dành cho thiết kế dược phẩm dựa trên cấu trúc hợp lý. Sau đó, AtomNet đã được sử dụng để dự đoán các phân tử sinh học được chọn mới lạ đối với nhiều mục tiêu bệnh tật, đặc biệt là phương pháp điều trị bệnh do virus Ebola và bệnh đa xơ cứng.


=== Quản lý quan hệ khách hàng (CRM) ===
Thành công gần đây đã được báo cáo với ứng dụng của học tăng cường sâu trong các thiết lập tiếp thị trực tiếp, thể hiện sự phù hợp của phương pháp này dành cho tự động hóa CRM. Một mạng nơ ron được sử dụng để ước tính giá trị của các hành động có thể trực tiếp tiếp thị trên không gian trạng thái khách hàng, được định nghĩa trong điều khoản của biến RFM. Hàm giá trị ước tính được chỉ ra để có một giải thích tự nhiên như là giá trị khách hàng suốt đời.


=== Các hệ thống khuyến cáo (gợi ý) ===
Các hệ thống khuyến cáo đã sử dụng học sâu để trích xuất các đặc điểm sâu có ý nghĩa cho mô hình yếu tố tiềm ẩn đối với khuyến cáo dựa trên nội dung cho âm nhạc. Gần đây, một cách tiếp cận tổng quát hơn cho việc học tập sở thích người dùng từ nhiều miền bằng cách sử dụng học sâu đa góc nhìn đã được đưa ra. Mô hình này sử dụng một cộng tác lai và tiếp cận dựa trên nội dung và tăng cường các khuyến nghị trong nhiều nhiệm vụ.


=== Tin sinh học ===
Gần đây, một cách tiếp cận học sâu dựa trên một mạng neuron nhân tạo tự mã hóa đã được sử dụng trong tin sinh học, để dự đoán các mối quan hệ chức năng gen và các chú thích Bản thể gen.


== Lý thuyết về bộ não con người ==
Tính toán học sâu có liên hệ chặt chẽ đến học thuyết về sự phát triển của não bộ (cụ thể, phát triển neocortical) do các nhà khoa học thần kinh nhận thức đề xuất trong đầu thập niên 1990. Một bản tóm tắt dễ tiếp cận của ý tưởng này là tác phẩm của Elman và các cộng sự vào năm 1996 "Xem xét lại Tính bẩm sinh" (Xem thêm: Shrager và Johnson; Quartz và Sejnowski). Những lý thuyết phát triển này cũng được thuyết minh cụ thể trong các mô hình tính toán, chúng là những kỹ thuật tiền nhiệm của các mô hình học sâu được thúc đẩy bởi tính toán (bằng máy tính) đơn thuần. Những mô hình phát triển này chia sẻ thuộc tính thú vị mà nhiều động lực học (learning dynamics) khác nhau được đề xuất trong nghiên cứu não bộ (Ví dụ, một làn sóng của yếu tố tăng trưởng thần kinh) để hỗ trợ việc tự tổ chức của các loại mạng nơ ron có liên quan với nhau được sử dụng trong các mô hình học sâu thuần tính toán sau đó; và các mạng neuron tính toán như vậy có vẻ tương tự như quan điểm của ngành nghiên cứu vỏ não mới như một hệ thống phân cấp của bộ lọc trong đó mỗi lớp chụp một số thông tin trong môi trường hoạt động, và sau đó đi qua phần còn lại, cũng như tín hiệu cơ bản được sửa đổi, tới các lớp khác cao hơn trong hệ thống phân cấp. Quá trình này mang lại một chồng tự tổ chức các cảm biến, cũng như điều chỉnh để hoạt động môi trường của họ. Như được mô tả trên tờ New York Times vào năm 1995: "...bộ não của những trẻ sơ sinh dường như tự tổ chức riêng chính nó dưới ảnh hưởng của các sóng của cái gọi là các yếu tố - dinh dưỡng... các khu vực khác nhau của não trở nên kết nối tuần tự, với một lớp mô trưởng thành trước các mô khác và cho đến khi toàn bộ não là trưởng thành."
Tầm quan trọng của học sâu đối với sự tiến hóa và phát triển của nhận thức của con người đã không thoát khỏi sự chú ý của các nhà nghiên cứu. Một khía cạnh của phát triển con người là phân biệt chúng ta với những người hàng xóm trong họ linh trưởng gần nhất của mình có thể thay đổi trong thời gian phát triển. Trong số các loài linh trưởng, bộ não con người vẫn còn tương đối mềm dẻo cho đến cuối thời kỳ sau khi sinh, trong khi bộ não của họ hàng gần gũi nhất của chúng ta hoàn toàn cố định hơn ngay sau khi sinh. Vì vậy, con người có khả năng truy cập lớn hơn vào những kinh nghiệm phức tạp đang diễn ra trên thế giới trong giai đoạn hình thành nhất của sự phát triển não bộ. Điều này có thể cho phép chúng ta "điều chỉnh" để thay đổi nhanh chóng môi trường mà các động vật khác, nhiều bị hạn chế bởi cơ cấu tiến hóa của bộ não của chúng, không thể để thực hiện được. Đến mức mà những thay đổi này được phản ánh trong các thay đổi thời gian tương tự trong sóng được giả thuyết của sự phát triển vỏ não, chúng cũng có thể dẫn đến những thay đổi trong việc khai thác thông tin từ môi trường kích thích trong thời gian đầu tự tổ chức của bộ não. Tất nhiên, cùng với tính linh hoạt này đến một giai đoạn kéo dài chưa thành thục, trong đó chúng ta phụ thuộc vào người chăm sóc và cộng đồng của mình để hỗ trợ và đào tạo. Lý thuyết của học sâu do đó thấy sự cùng tiến hóa đồng thời của văn hóa và nhận thức như là một điều kiện cơ bản của sự tiến hóa của con người.


== Hoạt động thương mại ==
Hầu hết các công ty công nghệ lớn nhất trên thế giới đang đầu tư rất nhiều nguồn lực vào nghiên cứu và phát triển để tiếp tục cải tiến công nghệ lõi cũng như tạo ra các sản phẩm ứng dụng sử dụng kỹ thuật học sâu. Điển hình là nhóm nghiên cứu về trí tuệ nhân tạo của Facebook đã tạo ra phần mềm DeepFace có khả năng nhận dạng khuôn mặt tốt như con người với độ chính xác khoảng 97,35%. Công trình này (công bố năm 2014) sử dụng 4 triệu ảnh khuôn mặt của hơn 4000 người để huấn luyện cho mạng nơron nhiều lớp và mô hình thu được đã vượt qua các kỹ thuật được nghiên cứu đề xuất trước đó.
Học sâu thường được trình bày như là một bước hướng tới AI mạnh và do đó nhiều tổ chức đã trở nên quan tâm đến việc sử dụng nó cho các ứng dụng cụ thể. Vào tháng 12 năm 2013, Facebook đã tuyển Yann Le Cun đứng đầu phòng thí nghiệm trí tuệ nhân tạo (AI) mới của họ hoạt động ở California, London và New York. Phòng thí nghiệm AI này sẽ phát triển những kỹ thuật học sâu để giúp Facebook thực hiện các nhiệm vụ, chẳng hạn như tính năng gắn thẻ tự động hình ảnh tải lên với tên của những người có mặt trong đó. Vào cuối năm 2014, Facebook cũng tuyển Vladimir Vapnik, nhà phát triển chính của lý thuyết Vapnik-Chervonenkis về học thống kê, và đồng phát minh ra phương pháp máy vector hỗ trợ.
Vào tháng 3 năm 2013, Google tuyển Geoffrey Hinton và hai sinh viên tốt nghiệp của ông, Alex Krizhevsky và Ilya Sutskever. Công việc của họ là tập trung vào vừa cải tiến các sản phẩm học máy hiện có của Google và vừa trợ giúp đối phó với lượng dữ liệu ngày càng tăng nhanh mà Google có được. Google cũng mua lại công ty của Hinton, DNNresearch.
Năm 2014, Google cũng đã mua DeepMind Technologies, một công ty khởi nghiệp của Anh đã phát triển một hệ thống có khả năng học tập làm thế nào để chơi trò chơi điện tử Atari chỉ sử dụng các điểm ảnh thô là dữ liệu đầu vào. Trong năm 2015, họ đã chứng minh hệ thống AlphaGo đã đạt được một trong những "thách thức lớn" trong thời gian dài của AI bằng cách học trò chơi Cờ vây đủ tốt để đánh bại một người chơi Cờ vây chuyên nghiệp.
Baidu đã thuê Andrew Ng để lãnh đạo phòng thí nghiệm nghiên cứu của mình đặt trụ sở tại thung lũng Silicon mới tập trung vào học sâu.


== Phê bình và đánh giá ==
neural Designer — Một ứng dụng GUI cho mạng neuron sâu cung cấp song song hóa với CPU.


== Tham khảo ==